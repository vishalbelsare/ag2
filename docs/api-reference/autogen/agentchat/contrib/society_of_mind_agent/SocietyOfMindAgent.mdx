---
sidebarTitle: SocietyOfMindAgent
title: autogen.agentchat.contrib.society_of_mind_agent.SocietyOfMindAgent
---
<h2 id="autogen.agentchat.contrib.society_of_mind_agent.SocietyOfMindAgent" class="doc doc-heading">
    <code class="doc-symbol doc-symbol-heading doc-symbol-class"></code>
    <span class="doc doc-object-name doc-class-name">SocietyOfMindAgent</span>
</h2>

```python
SocietyOfMindAgent(
    name: str,
    chat_manager: autogen.agentchat.groupchat.GroupChatManager,
    response_preparer: str | Callable | None = None,
    is_termination_msg: Callable[[dict], bool] | None = None,
    max_consecutive_auto_reply: int | None = None,
    human_input_mode: Literal['ALWAYS', 'NEVER', 'TERMINATE'] = 'TERMINATE',
    function_map: dict[str, typing.Callable] | None = None,
    code_execution_config: dict | Literal[False] = False,
    llm_config: dict | Literal[False] | None = False,
    default_auto_reply: dict | str | None = '',
    **kwargs
)
```

    (In preview) A single agent that runs a Group Chat as an inner monologue.<br/>At the end of the conversation (termination for any reason), the SocietyOfMindAgent
    applies the response_preparer method on the entire inner monologue message history to
    extract a final answer for the reply.<br/>Most arguments are inherited from ConversableAgent. New arguments are: <br/>    chat_manager (GroupChatManager): the group chat manager that will be running the inner monologue
        response_preparer (Optional, Callable or String): If response_preparer is a callable function, then
                it should have the signature: <br/>                f( self: SocietyOfMindAgent, messages: List[Dict])
                where `self` is this SocietyOfMindAgent, and `messages` is a list of inner-monologue messages.<br/>            The function should return a string representing the final response (extracted or prepared)
                from that history.<br/>            If response_preparer is a string, then it should be the LLM prompt used to extract the final
                message from the inner chat transcript.<br/>            The default response_preparer depends on if an llm_config is provided. If llm_config is False,
                then the response_preparer deterministically returns the last message in the inner-monolgue. If
                llm_config is set to anything else, then a default LLM prompt is used.<br/>

<b>Parameters:</b>
| Name | Description |
|--|--|
| `name` | **Type:** `str` |
| `chat_manager` | **Type:** `autogen.agentchat.groupchat.GroupChatManager` |
| `response_preparer` | **Type:** `str \| Callable \| None`<br/><br/>**Default:** None |
| `is_termination_msg` | **Type:** `Callable[[dict], bool] \| None`<br/><br/>**Default:** None |
| `max_consecutive_auto_reply` | **Type:** `int \| None`<br/><br/>**Default:** None |
| `human_input_mode` | **Type:** `Literal['ALWAYS', 'NEVER', 'TERMINATE']`<br/><br/>**Default:** 'TERMINATE' |
| `function_map` | **Type:** `dict[str, typing.Callable] \| None`<br/><br/>**Default:** None |
| `code_execution_config` | **Type:** `dict \| Literal[False]`<br/><br/>**Default:** False |
| `llm_config` | **Type:** `dict \| Literal[False] \| None`<br/><br/>**Default:** False |
| `default_auto_reply` | **Type:** `dict \| str \| None`<br/><br/>**Default:** '' |
| `**kwargs` |  |

### Instance Attributes

<code class="doc-symbol doc-symbol-heading doc-symbol-attribute"></code>
#### chat_manager
<br />

    Return the group chat manager.

### Instance Methods

<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>
#### generate_inner_monologue_reply

```python
generate_inner_monologue_reply(
    self,
    messages: list[dict] | None = None,
    sender: autogen.agentchat.agent.Agent | None = None,
    config: autogen.oai.client.OpenAIWrapper | None = None
) -> tuple[bool, str | dict | None]
```

    Generate a reply by running the group chat

<b>Parameters:</b>
| Name | Description |
|--|--|
| `messages` | **Type:** `list[dict] \| None`<br/><br/>**Default:** None |
| `sender` | **Type:** `autogen.agentchat.agent.Agent \| None`<br/><br/>**Default:** None |
| `config` | **Type:** `autogen.oai.client.OpenAIWrapper \| None`<br/><br/>**Default:** None |

<br />

<code class="doc-symbol doc-symbol-heading doc-symbol-method"></code>
#### update_chat_manager

```python
update_chat_manager(self, chat_manager: autogen.agentchat.groupchat.GroupChatManager | None) -> 
```

    Update the chat manager.<br/>

<b>Parameters:</b>
| Name | Description |
|--|--|
| `chat_manager` | the group chat manager<br/><br/>**Type:** `autogen.agentchat.groupchat.GroupChatManager \| None` |

<br />